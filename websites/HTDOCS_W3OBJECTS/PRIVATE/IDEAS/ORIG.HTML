<HTML>

<HEAD>
<TITLE>WebObjects: Ideas for WWW Research</TITLE>
<META NAME="AUTHOR" CONTENT="Dave Ingham">
</HEAD>

<BODY>

<CENTER>
<H1><A HREF="http://arjuna.ncl.ac.uk/webobjects">WebObjects</A></H1>
<H2>Ideas for WWW Research</H2>
<H4><I>Draft version, 14 January, 1995</I></H4>
</CENTER>
<HR>

<H2>Introduction </H2>

<P>
Desired final structure of document:
<OL>
<LI>Introduction - summarising the key point of applying object-oriented
technology to the web.
<LI>Description of the current web
<LI>Problems with the web
<LI>Object-oriented approach
<LI>Addressing some of the problems using the object-oriented
approach
<LI>Additional examples of what could be done in this framework
<LI><A NAME="_Toc314572698">Networked Information Publishing</A>

</OL>

<H2><A NAME="_Toc314572699"></A><A NAME="_Toc325021393">History</A>
</H2>

<P>
Until recently the methods used for publishing and retrieving
documentation and programs on the Internet were very primitive,
the most common being the file transfer protocol (FTP). This command-line
based tool allows a client to connect to a server which provides
access to a hierarchy of information, users can browse such hierarchies
using UNIX-like file commands such as <I>ls</I>, <I>cd</I> etc.
In addition clients are able to transfer items either from or
to the server using the <I>get</I> and <I>put</I> commands. Efficient
use of such tools requires knowledge of a multitude of access
methods and file types.
<H3><A NAME="_Toc314572700"></A><A NAME="_Toc325021394">Recent
Developments</A> </H3>

<P>
In the past few years, significant improvements have been made,
allowing networked information resources to be accessed through
more user-friendly graphical front-end interfaces. Three such
systems to have gained wide-spread acceptance are Gopher, Wide-Area
Information System (WAIS), and the World-Wide Web (WWW). Although
using different metaphors, each of these protocols provide the
same basic feature, allowing documents and other network entities
to be browsed and retrieved using an easy-to-use graphical interface.
Of the three systems, the World-Wide Web, which organises information
as a distributed hypertext, is arguably the richest of the current
systems.
<P>
[Description of the WWW Project; distributed hypertext, HTML,
HTTP, UR*]
<P>
[Current web browsers are able to access multiple protocols]
<H3><A NAME="_Toc314572701"></A><A NAME="_Toc325021395">Shortcomings
of Current Systems</A> </H3>

<P>
The advent of the world-wide web and the other systems previously
mentioned, has provided a significant step forward for networked
information publishing and has been successful in triggering the
imaginations of computer users from many different professions.
However, the web is still very much a developing technology and
in its current incarnation still suffers from a number of weaknesses.
These may be divided into three categories, firstly there are
the problems that users of the current system encounter as they
<I>surf the net</I>, secondly, there are problems of information
organisation and maintenance as faced by the information providers,
and thirdly, there are the limitations that the current system
imposes on the range of functionality that can be provided over
the web.
<P>
<A NAME="_Toc314572702">The User's Perspective</A> 
<OL>
<LI>The Missing-Link Problem
</OL>

<P>
The most annoying problem faced by web users today is the <I>missing-link</I>
problem, that is, as users view hypertext documents and click
on links that appeal to them, instead of the required information
appearing, an error message of the type shown in Figure 1 is displayed
by their web browser. This message is generated by the hypertext
transfer protocol daemon (HTTPD) at the server to indicate that
the selected information cannot be accessed. <HR>

<PRE>
The requested URL was not found
</PRE>
<HR>
<HR>

<PRE>
HTTP status code: 404 
</PRE>
<HR>

<P>
<B><A NAME="_Ref313935784">Figure 1</A>: Web browser error message</B>

<P>
Networked publishing sites currently tend to be very dynamic in
nature and are likely to remain so, indeed, one of the major advantages
of electronic publishing is the speed and ease by which information
can be made available. In this environment web resources are frequently
created, moved and destroyed in an ad-hoc manner suiting the owner
of the information or publishing site. These changes are usually
made autonomously, with little regard for how other users are
using the information. The fact that a user is holding a reference
to a resource, whether held within the <I>hotlist</I> of the user's
web browser or within a page of information that is currently
being viewed is no guarantee that the resource will be available
the next time an attempt is made to access it. Indeed, the definition
document for Uniform Resource Locators [ref] contains the following
text:
<P>
&quot;<I>Users should beware that there is no general guarantee
that a URL which at one time points to a given object continues
to do so, and does not even at some later time point to a different
object due to the movement of objects on servers</I>.&quot;
<P>
This is a problem of <I>referential integrity</I> and is the cause
of most user annoyance using the current web.
<OL>
<LI>Resource Unavailability
</OL>

<P>
[network partitions whether real or virtual may make certain resources
unavailable]
<P>
[justification for caching &amp; replication]
<OL>
<LI>Resource Responsiveness
</OL>

<P>
[even if a resource is contactable, the access may be very slow,
rendering the resource unusable, this is a common occurrence when
trying to access information from the USA during the afternoon
GMT. This is due to machine and network load and it is particular
problem with very popular resources.]
<P>
[justification for caching &amp; replication]
<P>
<A NAME="_Toc314572703">The Information Provider's Perspective</A>

<P>
Becoming a publisher on the web is a relatively straight forward
task, which accounts for the vast number of publishing sites that
exist around the world. The problems associated with electronic
publishing using the web lie in the organisation and maintenance
of the published information. For example to create a WWW publishing
site a HyperText Transfer Protocol Daemon (HTTPD) is installed
and configured on a machine connected to the network. Part of
the configuration process specifies a base directory from which
documents are served. Typically, the site maintainer creates a
hypertext index document in this base directory which provides
the entry point into the information structure and contains links
to the information published at that site. Such a server may be
used to publish traditional non-hypertext information, such as
a postscript version of this document, however, the real advantages
of the web are achieved by creating hypertext documents.
<P>
A well designed hypertext document provides many different ways
of traversing the information, allowing users to browse in a manner
that meets their specific needs. By adding hypertext anchors to
the document, related information can be provided at the exact
point of its relevance. A good example of the use of this technique
is in bibliographic reference lists, which can be made to be active
links to the documents in question, which may reside at different
publishing sites around the world. Being able to instantly follow
up any references to such associated information is one of the
major advantages of hypertext publishing.
<P>
<A NAME="_Toc314572704">Maintaining Consistency of Remote References</A>

<P>
One of the main problems facing an electronic information publisher
is the maintenance of the documents, in particular the references
to remote information. The web is a very large scale dynamic distributed
information system, where the control as well as the information
is distributed, with each site owner being responsible for the
creation, movement, and removal of documents.
<P>
When an author creates a hypertext document, inserting references
to associated pieces of remote information, the document provides
all the advantages previously mentioned. However, over the lifetime
of the document there are no guarantees that the owners of some
of the pieces of remote referenced information may not decide
to move or even delete the information, resulting in broken hypertext
links. This is again the referential integrity problem which is
the cause of browsing user's annoyance as mentioned in the previous
section. However, from the perspective of the information provider,
the cost of broken hypertext links is a tarnished reputation,
since browsing users will assuming it is the fault of the site
maintainer that a number of the references are broken.
<P>
The dynamic nature of the web therefore places a large burden
on the information provider in attempting to provide consistent
timely information, so as to maintain the reputation of the site.
Site maintainers have several options open to them for the discovery
of such broken links:
<UL>
<LI>The most obvious approach is not to maintain the web until
complaints are received from irate users, although because this
is such a large scale problem, users tend not to report such errors.
However, it is the reputation of the information provider that
is tarnished as he is seen to be providing a broken service, even
though he is not the originator of the problem.
</UL>

<UL>
<LI>Useful information as to the existence of broken hypertext
links can also be found be examining the logfiles that are generated
by the HTTPD which serves the documents. Error messages are logged
whenever a requested piece of information cannot be served. Broken
hypertext links within the information provider's documents will
cause such messages and highlight problems, but error messages
will also be created as a result of users typing errors in their
specification of resources. Sorting the relevant information from
the noise is a problem in itself.
</UL>

<UL>
<LI>Another option is for the site maintainer to periodically
perform an exhaustive traversal of the site, thereby detecting
any broken links. As the size and complexity of a site increases,
this becomes a laborious and time-consuming task.
</UL>

<UL>
<LI>The most effective way to detect problems is through the use
of a web-traversing robot, such as the Multi-Owner Maintenance
Spider (MOMSpider) [ref] which is capable to automatically traversing
web sites, generating information about broken links.
</UL>

<P>
After using one of the above techniques the maintainer has a list
of the broken links within the web site, but unfortunately this
information only provides part of the solution. In order to correct
the problems with the hypertext documents, the maintainer needs
to know the new location of the resources. Deducing this information
is less straight forward, the only options are to attempt to manually
examine the site from which the reference referred, hoping either
to find the resource or find an email address for a person who
may be able to provide the new location information. Clearly,
these solutions are far from ideal and result in web site maintainers
spending large amounts of time and effort in attempting to keep
their sites up-to-date.
<P>
<A NAME="_Toc314572705">Maintaining Local Resources</A> 
<P>
In addition to references to remote information, many local intra-document
and inter-document hypertext links will also exist within a information
publishing site. As stated previously, it is these links which
provide the advantages of hypertext publishing over traditional
one dimensional plain text or postscript documents. Intra-document
links are used for embedding graphical objects into documents
and for creating multiple routes for traversing documents. Inter-document
links are used for contents pages and for connecting related pieces
of local information. If all of the information published at a
particular site is controlled by the same person, then there the
problems of maintenance are reduced. However, a more typical scenario
is that web sites are used to publish information from a set of
individuals who individually author and maintain their documents,
resulting in similar problems that have been described in the
previous section albeit on a smaller scale.
<P>
<A NAME="_Toc314572706">Organising and Maintaining Published Information</A>

<P>
In effect there are two distinct interfaces to information published
using any of the electronic publishing techniques currently available,
from FTP servers to world-wide web sites. Firstly, there is the
interface provided to the external user for the browsing and retrieval
of information, and secondly there is the maintenance interface
that the site maintainer uses to structure and organise the information
to be served. This maintenance interface is typically the native
operating systems' file and directory commands, coupled with text
editing tools. For FTP there is a close connection between these
two interfaces, users are presented with the same hierarchical
view of the information and use similar commands for traversing
the structure, whether they are accessing the information remotely
through the FTP protocol or locally using the file system. However,
for the more recent systems, notably the WWW, the interfaces are
quite different, with two distinct structures existing in parallel,
the traditional hierarchical directory structure of the file system
and the complex interlinking web of hypertext. Within the hypertext
world resources are named using the URL scheme [ref], which consists
of an addressing scheme (protocol) followed by the some scheme
specific information, for example, for HTTP, the scheme specific
information (in the simplest case) consists of the Internet address
of the host on which the resource resides and the absolute file
system path of the resource relative to the base address of the
server. Figure 2 shows an example hypertext anchor, extracted
from an HTML document, showing a resource that is to be accessed
using the HTTP protocol from the host named <TT>www.cs.ncl.ac.uk</TT>,
held within the file <TT>index.html</TT> which is located in the
base directory of the server.<HR>

<PRE>
&lt;A HREF=&quot;http://www.cs.ncl.ac.uk/index.html&quot;&gt;Main Departmental server&lt;/A&gt;
</PRE>
<HR>

<P>
<B><A NAME="_Ref314559112">Figure 2</A>: Example hypertext anchor
using URL naming</B> 
<P>
Within a hypertext document, is also possible to use the Relative
URL [ref] scheme for embedding graphical objects or sub-sections
of a document. Here, only the relative path of the resource is
used, an example is shown in Figure 3. Here, since left unspecified,
the host is assumed to be the current host and the access mechanism
is assumed to be HTTP. The location of the document is specified
relative to the location of the current document in which the
reference exists.<HR>

<PRE>
&lt;A HREF=&quot;../arjuna/summary.html&quot;&gt;Project summary&lt;/A&gt;
</PRE>
<HR>

<P>
<B><A NAME="_Ref314559784">Figure 3</A>: Example hypertext anchor
using relative URL naming</B> 
<P>
Ignoring the problems of maintaining references to <I>remote</I>
information, other problems exist for a site owner even if none
of the published information contains any references to remote
information. The dynamic nature of electronic publishing means
that it is unlikely that the name and location of a resource,
allocated by its owner at the time of creation, will be appropriate
for the its lifetime. A common example of such dynamic changes
occur as publishing sites evolve; at the time of creation, a site
owner typically only has a limited number of resources to be published
and therefore creates links to these items from a top-level index
document. Each of the items may contain embedded graphical objects
and links to the other documents. Over the lifetime of the site,
more and more resources are added, until the owner decides that
it is appropriate to re-organise the information, maybe based
upon subject classification.
<P>
Due to the two distinct interfaces to the published information,
this re-organisation is likely to be required at two levels, requiring
changes to both the file system and the hypertext. At the file
system level it is likely that the directory hierarchy containing
the documents will be augmented with new directories and the resources
will be moved to the newly created directories as is appropriate.
These file system changes effectively mean that the names of the
resources are changing, which requires modifications to be made
at the hypertext level to reflect these new names, in practice
this means editing the hypertext anchors within the documents.
<P>
Furthermore, such organisational changes may improve the structure
of the local site but have the potential to break references at
remote sites, causing other site maintainers the headaches described
in the previous section. The HTTP protocol provides a limited
mechanism to minimise the effect on these remote sites by allowing
re-direction instructions placed at the old location of a resource
to point to its new location. So, in order to be a good web citizen,
the site maintainer is faced with the additional task of adding
all of the redirection pointers. Incidentally, since the maintainer
has no way of knowing whether all of the references to the old
resource locations have been updated and therefore can <I>never</I>
safely remove the redirection references.
<P>
The site maintainer has virtually no support in performing these
maintenance operations, the native operating systems' file commands
are used to perform the desired changes to the directory structure
and standard text editing tools have to be used to perform the
necessary changes to the hypertext documents. To complicate the
issue, during the re-configuration process, any browsing users
are likely to see a broken service, there is no standard mechanism
for the maintainer to be able to atomically switch from the old
structure to the new structure during a quiescent period.
<P>
In summary, although the hypertext interface to documents served
using the WWW provides an effective and exciting view of the published
information, the maintenance interface is inadequate for the tasks
that have to be performed, resulting in a large burden being placed
upon site maintainers.
<OL>
<LI>Garbage Collection
</OL>

<P>
[Every object cannot be kept for ever - doesn't scale. Therefore
providers need to know which resources are no longer required
and so may be backed out to secondary storage (CD-ROM or tape)
or even deleted. The HTTPD can provide statistics on resource
access, however, alone these are not enough to base the decision
upon. There may be certain resources to which important references
exist even though the resource is only accessed once per year
for example. Such objects definitely should not be deleted.]
<P>
[Justification for garbage collection mechanism based on referencing
as well as access patterns]
<P>
<A NAME="_Toc314572707">Functionality Limitations</A> 
<P>
The current world-wide web is primarily used for browsing and
reading documentation and in this regard it is a major success
story. Ignoring the problems mentioned in the previous sections,
the WWW has significantly improved the user's interface to electronically
published data. However, where the current web currently fails
is in the support for a richer set of operations to be performed
on more complex resources. The Common Gateway Interface (CGI)
[ref] does allow WWW front-ends to arbitrary user programs to
be created through the use of a forms interface, but provides
little infrastructure support for the creation of complex read-write
objects. The following sections describe several areas where it
is thought that the current infrastructure is lacking.
<P>
<A NAME="_Toc314572708">Access Control &amp; Security</A> 
<P>
In the current web framework, publishing is unrestrictive; either
a resource is private or it is readable by all. There are a wide
range of applications which require selective publishing, making
a resource available to a selected sub-set of users. For example,
such techniques would be required to allow companies to use the
web for collaborative editing project or remote access to sensitive
data. There is a need for a range of access control mechanisms
including access control lists and PGP encryption.
<OL>
<LI>Concurrency Control
</OL>

<P>
In order to allow more complex read-write resources on the web,
concurrency control becomes a fundamental requirement, so as to
ensure the consistency of information in the event of multiple
users attempting concurrent access to resources. Even for the
current predominately read-only resources, concurrency control
mechanisms would be useful at the times when the information providers
are updating or re-organising the published information.<A NAME="_Toc314572709"></A>

<P>
<A NAME="_Toc314572710">Fault-tolerance</A> 
<P>
Again, the fact that the current web is primarily concerned with
providing read-only access to resources means that there are limited
requirements for fault-tolerance. However, as soon as read-write
resources become more widespread, mechanisms, such as atomic transactions,
will be required so as to ensure the integrity of such resources
is maintained during write operations.
<P>
Scenarios are also likely to exist where the availability of resources
is of paramount importance. In these situations fault-tolerant
techniques will be required for failure masking and recovery.
<OL>
<LI>Replication
</OL>

<P>
For resources with high availability requirements, a single copy
of a resource is inappropriate. The research being carried out
by the Uniform Resource Identifier (URI) working group of the
Internet Engineering Task Force (IETF) into the use of location-independent
naming has considered the requirements for replicated resources.
Briefly, the concept is that a globally unique Uniform Resource
Name (URN) [ref], which identifies a logical resource can be resolved
into a Uniform Resource Characteristic (URC) [ref], which typically
contains a number of URLs, referring to individual copies of the
resource. A browser accessing a resource through a URN will retrieve
the URC through some lookup service and then choose the appropriate
URL to access. This scheme is focused on read-only resources;
there is no discussion presented in the documentation of how such
a scheme will ensure consistency of replicated resources in the
event of read-write operations.
<P>
It is difficult to see how replicated read-write objects could
be achieved using the current primary access protocol, HTTP.
<P>
<A NAME="_Toc314572711">Interoperability</A> 
<P>
The need for resources to be manipulated through a variety of
different interfaces is something that has not been considered
by the designers of the web. It is assumed that a resource will
only be accessed through the hypertext interface using the HTTPD.
Resources more complicated than textual documents are likely to
be accessed and updated through multiple interfaces. What is required
is a standard architecture, though which different clients may
access and update resources in a controlled manner, using concurrency
control mechanisms that belong to the resource rather than the
access mechanism.
<OL>
<LI>Proposed Research - An Object-oriented Approach
</OL>

<P>
Research work carried out within the distributed computing community,
over a period of many years, has addressed the difficulties observed
in the development of large scale computing systems. Such research
has lead to a detailed understanding of many of the issues that
have been highlighted as problems with the current World-Wide
Web. Examples of areas in which detailed research has been carried
out include scaleable naming [ANSA model ref], scaleable referencing
schemes [SSP chains ref], caching, replication, group communications,
and fault tolerance. Furthermore, a significant proportion of
this research has been carried out using the object-oriented paradigm,
in particular, within the domain of fine grained programming language-style
objects [Arjuna ref]. It is thought that the application of these
proven principles to the larger grained objects that reside on
the web is likely to provide informed solutions to some of the
current shortcomings, that were described in the previous section.
<P>
The remainder of this section presents some evidence indicating
why it is thought that the object-oriented paradigm is appropriate
for world-wide web development, highlighting some of the currently
hidden object-orientedness of the present system. What follows
is an initial attempt to formulate a set of basic properties that
are required by objects residing on the web, upon which more complex
functionality can be developed.
<OL>
<LI>Factors supporting an object-oriented approach
</OL>

<P>
[note form]
<P>
data + methods = objects (Encapsulation)
<P>
Current approach with regard to loading HTTPD with additional
functionality, e.g. searching methods, does not scale. Continuing
this approach will require HTTPD to contain code to implement
every method for every different type of web object. The object-oriented
solution would allow the objects to provide the methods appropriate
to themselves. Plug'n'play functionality - new object types would
require the installation of the methods for that type and not
a complete new monolithic HTTPD.
<P>
Polymorphism (Inheritance)
<P>
Standard methods are required by many different classes of web
object, for example migrate and display. However, the functionality
required varies depending upon the resource type. The polymorphic
property of the object-oriented paradigm would allow objects to
have a set of common operation interfaces, with alternative implementations
depending upon the object type.
<P>
Additional meta-data required in addition to text/HTML (Encapsulation)
<P>
Simple text or HTML files are not sufficient vehicles for holding
and manipulating the extra meta-information that is required by
web resources. An object-oriented approach would allow the meta-information
to be stored as member data together with the operations to support
its retrieval and manipulation.
<P>
Multiple Interfaces to Resources (Inheritance)
<P>
Resources require a range of different interfaces matching the
different ways in which they will be accessed, e.g. a HTML document
could have the following interfaces:
<P>
- standard hypertext display interface providing similar functionality
as is currently available
<P>
- a text only interface for serving browsers with limited display
capability
<P>
- a maintenance interface, providing operations such as migrate,
cache, and replicate
<P>
- interfaces to new access protocol superseding HTTP, to support
replication for example.
<P>
- interface serving more complex operations such as locking, versioning
etc.
<P>
- alternative interfaces for interoperability e.g. access through
a CORBA interface or an Arjuna interface
<P>
<A NAME="_Toc314572712">Core Web Object Properties</A> 
<P>
Within the proposed object-oriented framework, it is believed
that a number of fundamental properties are required by all classes
of objects that will reside on the web. For individual object
types, these fundamental properties will require specialisation
and will be supplemented by additional object-specific properties.
What follows is an attempt to identify and describe the set of
core object properties.<A NAME="_Toc314572713"></A> 
<P>
<A NAME="_Toc314572716">Naming</A> 
<P>
One of the fundamental concepts of the object-oriented paradigm
is identity. Without the ability to name documents they cannot
be referenced or accessed. Within the current web, the primary
naming scheme is the URL, which encodes both the location and
access protocol within the name. The IETF proposal for a globally
unique abstract name, the URN, is also expected to be accepted
by the Internet community. In order not to restrict interoperability,
the naming scheme within the web object framework must not prohibit
the use of such names. However, in terms of scalability and usability,
the proposed standards are lacking and it is likely that a context-relative
naming scheme will also be developed.
<OL>
<LI>Sharing
</OL>

<P>
It is implicit in the proposed application domain that objects
require the ability to be shared. A particular object may adopt
one of a number of sharing schemes depending upon the scale of
access that the owner of the object desires, for example, the
most basic sharing policy would be to allow all users to access
the object, as is the case within the current web. In addition,
there is also the requirement for more selective sharing between
restricted sets of users. Sharing policies based on access control
lists or PGP encryption are examples of what may be designed.
The concept of sharing also encompasses concurrency control, for
which a suite of policies will be required, for instance, access
based on multiple-reader, single-writer locking.
<P>
<A NAME="_Toc314572714">Mobility</A> 
<P>
Learning from the problems with the current web, it is believed
that the ability for an object to move from one location to another
is a necessary fundamental property of all web objects. Mobility
is a good example of how the use of object-oriented techniques
can lead to a more natural model, where a standard migrate interface
may exist for all object types, but the implementation will vary
depending upon the objects needs. For example, migration of an
HTML document requires additional functionality over the migration
of a postscript document, due to the necessary manipulation of
object's internal state as internal hypertext links are updated.
<P>
<A NAME="_Toc314572715">Referencing</A> 
<P>
In order to address the main problem of the current web, that
of referential integrity, referencing is thought to be a necessary
core property for web objects. The type of referencing scheme
adopted by an object will be effected by the availability and
consistency requirements of that object. A range of schemes are
possible, including forward referencing (SSP), call-backs, and
&quot;page fault and rebind through name server lookup&quot; [what
is the name for such a system?]. Referencing is closely related
to mobility, since referencing schemes are used to locate objects
even in the event of object migration.
<OL>
<LI>Higher-Level Object Properties
</OL>

<P>
The basic fundamental object properties of the previous section
form a base from which more sophisticated properties can be developed.
This section presents a selection of properties that are thought
to be applicable to a significant proportion of web objects that
either currently exist or are likely to exist in the future.
<P>
[These points require further work]
<OL>
<LI>Accounting
</OL>

<P>
Commercial use of the world-wide web is a development that is
sure to happen within a short period of time. It is likely that
certain resources will exist for which users will be require to
make a payment in order to access the resource. Realistically
speaking, charging will be most effective when applied to resources
that cannot easily be cached by users, such as rapidly changing
information, such as stock market data, or querying massive collections
of information, such as a search of the European registry of companies
in business. Irrespective of to which resources it applies, the
requirement for chargeable resources will arise.
<P>
As soon as users are charged for access to a resource the consequences
of errors within the system become more serious. For example,
consider the scenario where a user selects a link but only receives
part of the information before a problem at the server or on the
network causes the rest of the information to be lost. At present
such an occurrence would be annoying, but the user would be much
more irate if a payment had been made for the resource. Therefore,
accountable objects will require support for mechanisms such as
atomic transactions, so as to ensure that charging is performed
correctly, even in the event of failures.
<P>
[<I>Where should this go?</I> 
<P>
Users browsing the current web frequently access resources only
to subsequently discover that the information is of no interest
to them. One reason for this is that there is not enough information
associated with a link to indicate what lies at the end, another
reason is that the user may have to traverse several items of
uninteresting information before finding the link to the item
which is of interest. When users are paying for resources, then
they will pay more close attention to the information they retrieve.
Users will require the ability to traverse the <I>structure</I>
of the information without viewing the chargeable data. Such a
scheme would naturally fit into the web object framework, as such
meta-information would be stored within the object and the client
would be presented with a interface to retrieve such information
without having to retrieve the whole resource.]
<OL>
<LI>Versioning
</OL>

<P>
The concept of resource versioning is appropriate in a world where
resources are constantly changing. Typically, the nature of a
resource remains the same between versions and it is likely that
a general reference to a resource should refer to all versions
of that resources, although the ability to identify a specific
version is required. During resource development it is likely
that authors will want to permit different access restrictions
on the different versions. For example, after an author has created
a draft version of a document he may wish to publish his work
to a restricted set of users, while maintaining exclusive access
to a development version. Once complete, he may then wish to give
unrestricted access to the final version. A standard object versioning
interface to web objects would provide this functionality.
<OL>
<LI>Replication
</OL>

<P>
To address the problems of resource unavailability and responsiveness,
controlled replication of resources is required. A range of replication
schemes are necessary, for example, for read-only resources, a
read-from-any protocol is sufficient, as promoted by the URC/URN
work. However, for read-write resources, more complex replication
protocols are required to ensure object consistency. Depending
upon the resource type, the strictness of the consistency required
may vary, for example, it may be acceptable for certain replicas
of a given resource to drift slightly, whilst others may require
strict consistency guarantees.
<OL>
<LI>Caching
</OL>

<P>
Resource caching is another way to defeat responsiveness problems.
A cacheable resource must contain mechanisms to allow the cache
to be invalidated. The current scheme of allocating expiry dates
to resources for the purposing of caching is appropriate in some
cases. However, for other resources a more sophisticated caching
scheme, based on call-backs, is thought appropriate.
<OL>
<LI>Security
</OL>

<P>
[Discuss encryption]
<OL>
<LI>Fault tolerant
</OL>

<P>
[For high availability resources, again transactions, replication
etc.]
<P>
<A NAME="_Toc314572718">Addressing Current Problems</A> 
<P>
[Here's where we will outline more concrete proposals?]
<P>
<A NAME="_Toc314572719">Additional Possibilities In An Object-oriented
Framework</A> 
<P>
[notes]
<OL>
<LI>Maintenance Tool for Information Providers
</OL>

<P>
Graphical, file-manager-like tool allowing site maintainers to
organise published information and control the properties of objects
e.g. concurrency control policy or access control. The tool would
use the maintenance and control interfaces to the objects to automatically
modify hypertext links and perform the necessary work to ensure
that remote references are not broken. In addition the tool could
provide atomic update of the data.
<OL>
<LI>Collaborative Authoring Tool
</OL>

<P>
Collaborative editing is currently very difficult using current
web technology. However, such a tool would be implementable based
upon a web object framework supporting the proposed object properties
of concurrency control, controlled sharing, versioning and fault-tolerance.
<OL>
<LI>Synaptic Network
</OL>

<P>
Based upon neural net concepts, the basic idea is a dynamically
changing mesh of meta-objects (containing pointers to the real
objects), that re-configures itself based upon usage patterns
e.g. frequency accessed links would gain in strength and new links
between information would be created due to users access patterns.
A synaptic node shared between a group of similar-minded individuals
would allow important information to be highlighted.
<P>
<A NAME="_Toc314572720">References</A> 
<P>
The World Wide Web Project (original objectives document)
<P>
HTTP
<P>
HTML
<P>
Uniform Resource Locators (URL) - RFC1738 - December 1994
<P>
Relative URLs - IETF Draft
<P>
MOMSpider
<P>
Design &amp; Implementation of Arjuna
<P>
CORBA
<P>
World-Wide Web Proxies
<P>
Common Gateway Interface
<P>
URN
<P>
URC
<P>
<B>Spare Bits</B> 
<P>
Original list of web shortcomings:
<UL>
<LI>No referential integrity guarantees (no referencing mechanisms)
<LI>Independent document creation, naming and location
<LI>No access restrictions, concurrency control (currently primarily
read-only)
<LI>Unsophisticated management interface (file system)
<LI>Limited support for transparent migration, requires owner
expertise
<LI>No support for object replication
<LI>Limited inter-operability
<LI>Arbitrary organisation of information (may not match with
reader)
</UL>

<P>
[The crux of the proposed research is the application of the object-oriented
paradigm to the organisation and manipulation of the entities
that reside on the network. Through the object encapsulation of
web entities, multiple interfaces may be developed allowing more
sophisticated access and management operations. Furthermore, the
distributed computing community has addressed many of the problems
highlighted in the previous section in relation to fine grain
programming language objects. The application of these proven
principles to the larger grain web entities is likely to provide
informed solutions to some of the current shortcomings.]
<P>
[By publishing a piece of information on a web server, it instantly
changes from a private resource to a shared resource; a part of
a very large scale distributed information system. The owner of
a particular web server will organise the information in a manner
that he thinks is appropriate but this is unlikely to match the
needs of other users of this information. As a result, users commonly
make references to collections of information gleaned from distributed
sites to form an information structure that is geared to the needs
of an individual or group of users. However, although publishers
are free to make references to any published object on the web,
holding such a reference is no guarantee that the object will
be present the next time a user accesses the reference.]
<P>
<B>Referential integrity</B>:
<P>
- if a client detects follows a redirected link, a message could
be sent to the HTTPD from which the reference was obtained indicating
that a resource has moved from one location to another. The daemon
could verify the information by accessing the link itself. If
the hint was found to be correct then the daemon could automatically
update that information in all documents that it was serving.
(Stuart).
<P>
- Cast into the Steve's garbage collection framework - Users detecting
bad references is a safety problem while providers not knowing
when it is safe to delete objects is a liveness problem. 
</BODY>

</HTML>
